{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8991432,"sourceType":"datasetVersion","datasetId":5415659}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T13:07:43.280792Z","iopub.execute_input":"2024-07-19T13:07:43.281129Z","iopub.status.idle":"2024-07-19T13:07:44.300369Z","shell.execute_reply.started":"2024-07-19T13:07:43.281104Z","shell.execute_reply":"2024-07-19T13:07:44.299452Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bolbolzaban/test.txt\n/kaggle/input/bolbolzaban/train.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\n\nfile_path = '/kaggle/input/bolbolzaban/train.txt'  \nwith open(file_path, 'r', encoding='utf-8') as f:\n    texts = f.readlines()\n\ntokenizer = AutoTokenizer.from_pretrained(\"bolbolzaban/gpt2-persian\")\n\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n\nclass PoetryDataset(Dataset):\n    def __init__(self, texts, tokenizer, block_size):\n        self.tokenizer = tokenizer\n        self.block_size = block_size\n        self.data = []\n\n        for text in texts:\n            tokenized_text = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.block_size+1, return_tensors='pt')\n            input_ids = tokenized_text['input_ids'][0]\n            self.data.append(input_ids)\n        self.data = torch.stack(self.data)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        chunk = self.data[idx]\n        return chunk[:-1], chunk[1:]\n\nblock_size = 256  \ndataset = PoetryDataset(texts, tokenizer, block_size)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:07:58.409456Z","iopub.execute_input":"2024-07-19T13:07:58.410264Z","iopub.status.idle":"2024-07-19T13:08:43.199581Z","shell.execute_reply.started":"2024-07-19T13:07:58.410232Z","shell.execute_reply":"2024-07-19T13:08:43.198735Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f895e3216fe40008a9e4fb503235172"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f223ef3874f4d2a86a29db78ae5354b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/537k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31092d2812484f62ad31a68dc54d048c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3afa7953cee44c018c57747b58b8d3fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/399 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb376e3be6745ea9432232aa2a1fd68"}},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n        self.drop = nn.Dropout(0.1)\n        self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n\n    def forward(self, idx, targets=None):\n        b, t = idx.size()\n        assert t <= block_size, \"Cannot forward, model block size is exhausted.\"\n        token_embeddings = self.tok_emb(idx)\n        position_embeddings = self.pos_emb[:, :t, :]\n        x = self.drop(token_embeddings + position_embeddings)\n        for block in self.blocks:\n            x = block(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n            return logits, loss\n        return logits\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.attn = nn.MultiheadAttention(n_embd, n_head, dropout=0.1)\n        self.ln2 = nn.LayerNorm(n_embd)\n        self.mlp = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(0.1),\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x), need_weights=False)[0]\n        x = x + self.mlp(self.ln2(x))\n        return x\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nvocab_size = tokenizer.vocab_size\nn_embd = 256\nn_head = 8\nn_layer = 6\nmodel = GPT(vocab_size, n_embd, n_head, n_layer, block_size).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:11:53.435366Z","iopub.execute_input":"2024-07-19T13:11:53.436166Z","iopub.status.idle":"2024-07-19T13:11:53.648046Z","shell.execute_reply.started":"2024-07-19T13:11:53.436134Z","shell.execute_reply":"2024-07-19T13:11:53.647107Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    for idx, (input_ids, targets) in enumerate(dataloader):\n        input_ids, targets = input_ids.to(device), targets.to(device)\n\n        logits, loss = model(input_ids, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if idx % 100 == 0:\n            print(f\"Epoch {epoch + 1}, Iteration {idx}, Loss: {loss.item()}\")\n\nmodel_save_path = './gpt_persian_poetry_model.pth'\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:57:24.495888Z","iopub.execute_input":"2024-07-19T14:57:24.496589Z","iopub.status.idle":"2024-07-19T15:49:42.457570Z","shell.execute_reply.started":"2024-07-19T14:57:24.496556Z","shell.execute_reply":"2024-07-19T15:49:42.456488Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1, Iteration 0, Loss: 0.30869245529174805\nEpoch 1, Iteration 100, Loss: 0.3019784092903137\nEpoch 1, Iteration 200, Loss: 0.29770228266716003\nEpoch 1, Iteration 300, Loss: 0.3079856038093567\nEpoch 1, Iteration 400, Loss: 0.3364007771015167\nEpoch 1, Iteration 500, Loss: 0.2769355773925781\nEpoch 1, Iteration 600, Loss: 0.31606677174568176\nEpoch 1, Iteration 700, Loss: 0.2782537639141083\nEpoch 1, Iteration 800, Loss: 0.3062870502471924\nEpoch 1, Iteration 900, Loss: 0.32673779129981995\nEpoch 1, Iteration 1000, Loss: 0.3364282250404358\nEpoch 1, Iteration 1100, Loss: 0.3094336688518524\nEpoch 1, Iteration 1200, Loss: 0.32172006368637085\nEpoch 1, Iteration 1300, Loss: 0.30913376808166504\nEpoch 1, Iteration 1400, Loss: 0.321206271648407\nEpoch 1, Iteration 1500, Loss: 0.3021668493747711\nEpoch 1, Iteration 1600, Loss: 0.3502541184425354\nEpoch 1, Iteration 1700, Loss: 0.32372191548347473\nEpoch 1, Iteration 1800, Loss: 0.27154988050460815\nEpoch 1, Iteration 1900, Loss: 0.3430746793746948\nEpoch 1, Iteration 2000, Loss: 0.3198419511318207\nEpoch 1, Iteration 2100, Loss: 0.3041294813156128\nEpoch 1, Iteration 2200, Loss: 0.2752663195133209\nEpoch 1, Iteration 2300, Loss: 0.3131007254123688\nEpoch 1, Iteration 2400, Loss: 0.3298114836215973\nEpoch 1, Iteration 2500, Loss: 0.3301908075809479\nEpoch 1, Iteration 2600, Loss: 0.3162791430950165\nEpoch 1, Iteration 2700, Loss: 0.31374719738960266\nEpoch 1, Iteration 2800, Loss: 0.32129162549972534\nEpoch 1, Iteration 2900, Loss: 0.31040704250335693\nEpoch 1, Iteration 3000, Loss: 0.29519787430763245\nEpoch 1, Iteration 3100, Loss: 0.3379867374897003\nEpoch 1, Iteration 3200, Loss: 0.3085896372795105\nEpoch 1, Iteration 3300, Loss: 0.3010779321193695\nEpoch 1, Iteration 3400, Loss: 0.34485942125320435\nEpoch 1, Iteration 3500, Loss: 0.33452412486076355\nEpoch 1, Iteration 3600, Loss: 0.2819323241710663\nEpoch 1, Iteration 3700, Loss: 0.3329557478427887\nEpoch 1, Iteration 3800, Loss: 0.2795126438140869\nEpoch 1, Iteration 3900, Loss: 0.3249654471874237\nEpoch 1, Iteration 4000, Loss: 0.3293689489364624\nEpoch 1, Iteration 4100, Loss: 0.32560795545578003\nEpoch 1, Iteration 4200, Loss: 0.28822267055511475\nEpoch 1, Iteration 4300, Loss: 0.31662827730178833\nEpoch 1, Iteration 4400, Loss: 0.319938600063324\nEpoch 1, Iteration 4500, Loss: 0.32848528027534485\nEpoch 2, Iteration 0, Loss: 0.27568212151527405\nEpoch 2, Iteration 100, Loss: 0.29901790618896484\nEpoch 2, Iteration 200, Loss: 0.30183106660842896\nEpoch 2, Iteration 300, Loss: 0.31878480315208435\nEpoch 2, Iteration 400, Loss: 0.2923969328403473\nEpoch 2, Iteration 500, Loss: 0.3037889897823334\nEpoch 2, Iteration 600, Loss: 0.2879246771335602\nEpoch 2, Iteration 700, Loss: 0.30814439058303833\nEpoch 2, Iteration 800, Loss: 0.2922709882259369\nEpoch 2, Iteration 900, Loss: 0.2967361807823181\nEpoch 2, Iteration 1000, Loss: 0.30977797508239746\nEpoch 2, Iteration 1100, Loss: 0.3038484752178192\nEpoch 2, Iteration 1200, Loss: 0.31430983543395996\nEpoch 2, Iteration 1300, Loss: 0.29622533917427063\nEpoch 2, Iteration 1400, Loss: 0.28176015615463257\nEpoch 2, Iteration 1500, Loss: 0.3088594377040863\nEpoch 2, Iteration 1600, Loss: 0.33529001474380493\nEpoch 2, Iteration 1700, Loss: 0.3023328185081482\nEpoch 2, Iteration 1800, Loss: 0.30840760469436646\nEpoch 2, Iteration 1900, Loss: 0.29878294467926025\nEpoch 2, Iteration 2000, Loss: 0.3059433102607727\nEpoch 2, Iteration 2100, Loss: 0.29647165536880493\nEpoch 2, Iteration 2200, Loss: 0.3248760402202606\nEpoch 2, Iteration 2300, Loss: 0.30990079045295715\nEpoch 2, Iteration 2400, Loss: 0.3139788806438446\nEpoch 2, Iteration 2500, Loss: 0.32025375962257385\nEpoch 2, Iteration 2600, Loss: 0.3020159900188446\nEpoch 2, Iteration 2700, Loss: 0.3305116295814514\nEpoch 2, Iteration 2800, Loss: 0.3492010235786438\nEpoch 2, Iteration 2900, Loss: 0.316319078207016\nEpoch 2, Iteration 3000, Loss: 0.29909729957580566\nEpoch 2, Iteration 3100, Loss: 0.3504830002784729\nEpoch 2, Iteration 3200, Loss: 0.3075803220272064\nEpoch 2, Iteration 3300, Loss: 0.3162476122379303\nEpoch 2, Iteration 3400, Loss: 0.2644194960594177\nEpoch 2, Iteration 3500, Loss: 0.2920275926589966\nEpoch 2, Iteration 3600, Loss: 0.3390098214149475\nEpoch 2, Iteration 3700, Loss: 0.30099666118621826\nEpoch 2, Iteration 3800, Loss: 0.3076767325401306\nEpoch 2, Iteration 3900, Loss: 0.32742637395858765\nEpoch 2, Iteration 4000, Loss: 0.3142227232456207\nEpoch 2, Iteration 4100, Loss: 0.29102280735969543\nEpoch 2, Iteration 4200, Loss: 0.3121979534626007\nEpoch 2, Iteration 4300, Loss: 0.3306935131549835\nEpoch 2, Iteration 4400, Loss: 0.30708155035972595\nEpoch 2, Iteration 4500, Loss: 0.3017100691795349\nEpoch 3, Iteration 0, Loss: 0.3115812838077545\nEpoch 3, Iteration 100, Loss: 0.3515869081020355\nEpoch 3, Iteration 200, Loss: 0.287864089012146\nEpoch 3, Iteration 300, Loss: 0.31250789761543274\nEpoch 3, Iteration 400, Loss: 0.27880582213401794\nEpoch 3, Iteration 500, Loss: 0.3341813385486603\nEpoch 3, Iteration 600, Loss: 0.2803632318973541\nEpoch 3, Iteration 700, Loss: 0.2963216304779053\nEpoch 3, Iteration 800, Loss: 0.31732290983200073\nEpoch 3, Iteration 900, Loss: 0.280121773481369\nEpoch 3, Iteration 1000, Loss: 0.3073635399341583\nEpoch 3, Iteration 1100, Loss: 0.32000088691711426\nEpoch 3, Iteration 1200, Loss: 0.31959661841392517\nEpoch 3, Iteration 1300, Loss: 0.30586355924606323\nEpoch 3, Iteration 1400, Loss: 0.31102779507637024\nEpoch 3, Iteration 1500, Loss: 0.31091177463531494\nEpoch 3, Iteration 1600, Loss: 0.29624244570732117\nEpoch 3, Iteration 1700, Loss: 0.3260916471481323\nEpoch 3, Iteration 1800, Loss: 0.32386404275894165\nEpoch 3, Iteration 1900, Loss: 0.2952147126197815\nEpoch 3, Iteration 2000, Loss: 0.3025997579097748\nEpoch 3, Iteration 2100, Loss: 0.3236680030822754\nEpoch 3, Iteration 2200, Loss: 0.30196613073349\nEpoch 3, Iteration 2300, Loss: 0.30019611120224\nEpoch 3, Iteration 2400, Loss: 0.27991586923599243\nEpoch 3, Iteration 2500, Loss: 0.2970707416534424\nEpoch 3, Iteration 2600, Loss: 0.35129013657569885\nEpoch 3, Iteration 2700, Loss: 0.3114525079727173\nEpoch 3, Iteration 2800, Loss: 0.28553467988967896\nEpoch 3, Iteration 2900, Loss: 0.2937771677970886\nEpoch 3, Iteration 3000, Loss: 0.3221009075641632\nEpoch 3, Iteration 3100, Loss: 0.2949144244194031\nEpoch 3, Iteration 3200, Loss: 0.32787439227104187\nEpoch 3, Iteration 3300, Loss: 0.2755141258239746\nEpoch 3, Iteration 3400, Loss: 0.3111357092857361\nEpoch 3, Iteration 3500, Loss: 0.32065409421920776\nEpoch 3, Iteration 3600, Loss: 0.3442123532295227\nEpoch 3, Iteration 3700, Loss: 0.3091278374195099\nEpoch 3, Iteration 3800, Loss: 0.3165663182735443\nEpoch 3, Iteration 3900, Loss: 0.3408627510070801\nEpoch 3, Iteration 4000, Loss: 0.3128512501716614\nEpoch 3, Iteration 4100, Loss: 0.3114570081233978\nEpoch 3, Iteration 4200, Loss: 0.3392155170440674\nEpoch 3, Iteration 4300, Loss: 0.33425766229629517\nEpoch 3, Iteration 4400, Loss: 0.2826961576938629\nEpoch 3, Iteration 4500, Loss: 0.33144399523735046\nEpoch 4, Iteration 0, Loss: 0.2949489653110504\nEpoch 4, Iteration 100, Loss: 0.3108935058116913\nEpoch 4, Iteration 200, Loss: 0.278482049703598\nEpoch 4, Iteration 300, Loss: 0.2982944846153259\nEpoch 4, Iteration 400, Loss: 0.34870946407318115\nEpoch 4, Iteration 500, Loss: 0.31451791524887085\nEpoch 4, Iteration 600, Loss: 0.31636282801628113\nEpoch 4, Iteration 700, Loss: 0.31293371319770813\nEpoch 4, Iteration 800, Loss: 0.30503469705581665\nEpoch 4, Iteration 900, Loss: 0.2988324463367462\nEpoch 4, Iteration 1000, Loss: 0.3008226156234741\nEpoch 4, Iteration 1100, Loss: 0.2867107093334198\nEpoch 4, Iteration 1200, Loss: 0.2911982834339142\nEpoch 4, Iteration 1300, Loss: 0.3438795208930969\nEpoch 4, Iteration 1400, Loss: 0.3015749454498291\nEpoch 4, Iteration 1500, Loss: 0.30100324749946594\nEpoch 4, Iteration 1600, Loss: 0.3215838372707367\nEpoch 4, Iteration 1700, Loss: 0.31020545959472656\nEpoch 4, Iteration 1800, Loss: 0.3211628198623657\nEpoch 4, Iteration 1900, Loss: 0.3005671799182892\nEpoch 4, Iteration 2000, Loss: 0.3091263771057129\nEpoch 4, Iteration 2100, Loss: 0.3185616731643677\nEpoch 4, Iteration 2200, Loss: 0.30611008405685425\nEpoch 4, Iteration 2300, Loss: 0.30626827478408813\nEpoch 4, Iteration 2400, Loss: 0.2945743799209595\nEpoch 4, Iteration 2500, Loss: 0.3296591639518738\nEpoch 4, Iteration 2600, Loss: 0.3307574391365051\nEpoch 4, Iteration 2700, Loss: 0.3105371594429016\nEpoch 4, Iteration 2800, Loss: 0.3326863646507263\nEpoch 4, Iteration 2900, Loss: 0.32325515151023865\nEpoch 4, Iteration 3000, Loss: 0.3354617953300476\nEpoch 4, Iteration 3100, Loss: 0.306591659784317\nEpoch 4, Iteration 3200, Loss: 0.29975125193595886\nEpoch 4, Iteration 3300, Loss: 0.30538350343704224\nEpoch 4, Iteration 3400, Loss: 0.3113318085670471\nEpoch 4, Iteration 3500, Loss: 0.2927217185497284\nEpoch 4, Iteration 3600, Loss: 0.2956441640853882\nEpoch 4, Iteration 3700, Loss: 0.29855042695999146\nEpoch 4, Iteration 3800, Loss: 0.33009809255599976\nEpoch 4, Iteration 3900, Loss: 0.3176578879356384\nEpoch 4, Iteration 4000, Loss: 0.31130173802375793\nEpoch 4, Iteration 4100, Loss: 0.283332496881485\nEpoch 4, Iteration 4200, Loss: 0.31722331047058105\nEpoch 4, Iteration 4300, Loss: 0.3013104498386383\nEpoch 4, Iteration 4400, Loss: 0.3249783217906952\nEpoch 4, Iteration 4500, Loss: 0.2907145619392395\nEpoch 5, Iteration 0, Loss: 0.318642258644104\nEpoch 5, Iteration 100, Loss: 0.30694738030433655\nEpoch 5, Iteration 200, Loss: 0.31138235330581665\nEpoch 5, Iteration 300, Loss: 0.30702176690101624\nEpoch 5, Iteration 400, Loss: 0.31433650851249695\nEpoch 5, Iteration 500, Loss: 0.30673885345458984\nEpoch 5, Iteration 600, Loss: 0.314860463142395\nEpoch 5, Iteration 700, Loss: 0.28022587299346924\nEpoch 5, Iteration 800, Loss: 0.3024923503398895\nEpoch 5, Iteration 900, Loss: 0.3013243079185486\nEpoch 5, Iteration 1000, Loss: 0.27749794721603394\nEpoch 5, Iteration 1100, Loss: 0.32269179821014404\nEpoch 5, Iteration 1200, Loss: 0.29810789227485657\nEpoch 5, Iteration 1300, Loss: 0.3218616843223572\nEpoch 5, Iteration 1400, Loss: 0.285418838262558\nEpoch 5, Iteration 1500, Loss: 0.2998526692390442\nEpoch 5, Iteration 1600, Loss: 0.33781468868255615\nEpoch 5, Iteration 1700, Loss: 0.2829830050468445\nEpoch 5, Iteration 1800, Loss: 0.31310445070266724\nEpoch 5, Iteration 1900, Loss: 0.31355947256088257\nEpoch 5, Iteration 2000, Loss: 0.3261412978172302\nEpoch 5, Iteration 2100, Loss: 0.28883787989616394\nEpoch 5, Iteration 2200, Loss: 0.32070067524909973\nEpoch 5, Iteration 2300, Loss: 0.31135740876197815\nEpoch 5, Iteration 2400, Loss: 0.29499074816703796\nEpoch 5, Iteration 2500, Loss: 0.3224407434463501\nEpoch 5, Iteration 2600, Loss: 0.3240020275115967\nEpoch 5, Iteration 2700, Loss: 0.3099842369556427\nEpoch 5, Iteration 2800, Loss: 0.3131972551345825\nEpoch 5, Iteration 2900, Loss: 0.30944928526878357\nEpoch 5, Iteration 3000, Loss: 0.3103032410144806\nEpoch 5, Iteration 3100, Loss: 0.3113914132118225\nEpoch 5, Iteration 3200, Loss: 0.29212310910224915\nEpoch 5, Iteration 3300, Loss: 0.2944226562976837\nEpoch 5, Iteration 3400, Loss: 0.318172425031662\nEpoch 5, Iteration 3500, Loss: 0.318563848733902\nEpoch 5, Iteration 3600, Loss: 0.3429265022277832\nEpoch 5, Iteration 3700, Loss: 0.32930895686149597\nEpoch 5, Iteration 3800, Loss: 0.36087265610694885\nEpoch 5, Iteration 3900, Loss: 0.31070563197135925\nEpoch 5, Iteration 4000, Loss: 0.30744805932044983\nEpoch 5, Iteration 4100, Loss: 0.3350307047367096\nEpoch 5, Iteration 4200, Loss: 0.29095613956451416\nEpoch 5, Iteration 4300, Loss: 0.2956286072731018\nEpoch 5, Iteration 4400, Loss: 0.30244019627571106\nEpoch 5, Iteration 4500, Loss: 0.3056183159351349\nEpoch 6, Iteration 0, Loss: 0.34414854645729065\nEpoch 6, Iteration 100, Loss: 0.31171801686286926\nEpoch 6, Iteration 200, Loss: 0.30067458748817444\nEpoch 6, Iteration 300, Loss: 0.297156423330307\nEpoch 6, Iteration 400, Loss: 0.2989889681339264\nEpoch 6, Iteration 500, Loss: 0.31176501512527466\nEpoch 6, Iteration 600, Loss: 0.29820358753204346\nEpoch 6, Iteration 700, Loss: 0.3341924250125885\nEpoch 6, Iteration 800, Loss: 0.3430427014827728\nEpoch 6, Iteration 900, Loss: 0.2783356010913849\nEpoch 6, Iteration 1000, Loss: 0.33471739292144775\nEpoch 6, Iteration 1100, Loss: 0.3011603355407715\nEpoch 6, Iteration 1200, Loss: 0.274241179227829\nEpoch 6, Iteration 1300, Loss: 0.29069769382476807\nEpoch 6, Iteration 1400, Loss: 0.2858532667160034\nEpoch 6, Iteration 1500, Loss: 0.3342245817184448\nEpoch 6, Iteration 1600, Loss: 0.3183000385761261\nEpoch 6, Iteration 1700, Loss: 0.29320085048675537\nEpoch 6, Iteration 1800, Loss: 0.2730772793292999\nEpoch 6, Iteration 1900, Loss: 0.33435937762260437\nEpoch 6, Iteration 2000, Loss: 0.31044670939445496\nEpoch 6, Iteration 2100, Loss: 0.27823129296302795\nEpoch 6, Iteration 2200, Loss: 0.3039010167121887\nEpoch 6, Iteration 2300, Loss: 0.3056296706199646\nEpoch 6, Iteration 2400, Loss: 0.2959059774875641\nEpoch 6, Iteration 2500, Loss: 0.30659019947052\nEpoch 6, Iteration 2600, Loss: 0.34602078795433044\nEpoch 6, Iteration 2700, Loss: 0.2867412865161896\nEpoch 6, Iteration 2800, Loss: 0.2904767394065857\nEpoch 6, Iteration 2900, Loss: 0.306326299905777\nEpoch 6, Iteration 3000, Loss: 0.311202734708786\nEpoch 6, Iteration 3100, Loss: 0.29779019951820374\nEpoch 6, Iteration 3200, Loss: 0.34166550636291504\nEpoch 6, Iteration 3300, Loss: 0.30838143825531006\nEpoch 6, Iteration 3400, Loss: 0.2797708511352539\nEpoch 6, Iteration 3500, Loss: 0.3127795457839966\nEpoch 6, Iteration 3600, Loss: 0.31334254145622253\nEpoch 6, Iteration 3700, Loss: 0.30939191579818726\nEpoch 6, Iteration 3800, Loss: 0.3140231966972351\nEpoch 6, Iteration 3900, Loss: 0.3194962441921234\nEpoch 6, Iteration 4000, Loss: 0.27461931109428406\nEpoch 6, Iteration 4100, Loss: 0.29208606481552124\nEpoch 6, Iteration 4200, Loss: 0.30062076449394226\nEpoch 6, Iteration 4300, Loss: 0.30513280630111694\nEpoch 6, Iteration 4400, Loss: 0.303803026676178\nEpoch 6, Iteration 4500, Loss: 0.28902024030685425\nEpoch 7, Iteration 0, Loss: 0.30162477493286133\nEpoch 7, Iteration 100, Loss: 0.24591699242591858\nEpoch 7, Iteration 200, Loss: 0.2921505272388458\nEpoch 7, Iteration 300, Loss: 0.3081755042076111\nEpoch 7, Iteration 400, Loss: 0.2994258999824524\nEpoch 7, Iteration 500, Loss: 0.3010581433773041\nEpoch 7, Iteration 600, Loss: 0.31105485558509827\nEpoch 7, Iteration 700, Loss: 0.2895541787147522\nEpoch 7, Iteration 800, Loss: 0.28072431683540344\nEpoch 7, Iteration 900, Loss: 0.2824978530406952\nEpoch 7, Iteration 1000, Loss: 0.31895312666893005\nEpoch 7, Iteration 1100, Loss: 0.3032907247543335\nEpoch 7, Iteration 1200, Loss: 0.32702216506004333\nEpoch 7, Iteration 1300, Loss: 0.2982396185398102\nEpoch 7, Iteration 1400, Loss: 0.3190631866455078\nEpoch 7, Iteration 1500, Loss: 0.2959877550601959\nEpoch 7, Iteration 1600, Loss: 0.31349435448646545\nEpoch 7, Iteration 1700, Loss: 0.2968778908252716\nEpoch 7, Iteration 1800, Loss: 0.32189449667930603\nEpoch 7, Iteration 1900, Loss: 0.2760036885738373\nEpoch 7, Iteration 2000, Loss: 0.28135040402412415\nEpoch 7, Iteration 2100, Loss: 0.31324291229248047\nEpoch 7, Iteration 2200, Loss: 0.3191440999507904\nEpoch 7, Iteration 2300, Loss: 0.29376518726348877\nEpoch 7, Iteration 2400, Loss: 0.2956773042678833\nEpoch 7, Iteration 2500, Loss: 0.29496923089027405\nEpoch 7, Iteration 2600, Loss: 0.31384173035621643\nEpoch 7, Iteration 2700, Loss: 0.30640909075737\nEpoch 7, Iteration 2800, Loss: 0.26952168345451355\nEpoch 7, Iteration 2900, Loss: 0.321683406829834\nEpoch 7, Iteration 3000, Loss: 0.2926270365715027\nEpoch 7, Iteration 3100, Loss: 0.33931034803390503\nEpoch 7, Iteration 3200, Loss: 0.3020269572734833\nEpoch 7, Iteration 3300, Loss: 0.27371883392333984\nEpoch 7, Iteration 3400, Loss: 0.28738126158714294\nEpoch 7, Iteration 3500, Loss: 0.3144989311695099\nEpoch 7, Iteration 3600, Loss: 0.3073965311050415\nEpoch 7, Iteration 3700, Loss: 0.2888965904712677\nEpoch 7, Iteration 3800, Loss: 0.3004046678543091\nEpoch 7, Iteration 3900, Loss: 0.3047373294830322\nEpoch 7, Iteration 4000, Loss: 0.29343587160110474\nEpoch 7, Iteration 4100, Loss: 0.30443671345710754\nEpoch 7, Iteration 4200, Loss: 0.3232990801334381\nEpoch 7, Iteration 4300, Loss: 0.32269367575645447\nEpoch 7, Iteration 4400, Loss: 0.3162672817707062\nEpoch 7, Iteration 4500, Loss: 0.3206525146961212\nEpoch 8, Iteration 0, Loss: 0.29696375131607056\nEpoch 8, Iteration 100, Loss: 0.3045594394207001\nEpoch 8, Iteration 200, Loss: 0.32851332426071167\nEpoch 8, Iteration 300, Loss: 0.29530537128448486\nEpoch 8, Iteration 400, Loss: 0.3267773985862732\nEpoch 8, Iteration 500, Loss: 0.2976972162723541\nEpoch 8, Iteration 600, Loss: 0.33788228034973145\nEpoch 8, Iteration 700, Loss: 0.3059743344783783\nEpoch 8, Iteration 800, Loss: 0.28166234493255615\nEpoch 8, Iteration 900, Loss: 0.30400019884109497\nEpoch 8, Iteration 1000, Loss: 0.30345025658607483\nEpoch 8, Iteration 1100, Loss: 0.28918489813804626\nEpoch 8, Iteration 1200, Loss: 0.2865282893180847\nEpoch 8, Iteration 1300, Loss: 0.2734382748603821\nEpoch 8, Iteration 1400, Loss: 0.31739121675491333\nEpoch 8, Iteration 1500, Loss: 0.29038169980049133\nEpoch 8, Iteration 1600, Loss: 0.3165495991706848\nEpoch 8, Iteration 1700, Loss: 0.2759036123752594\nEpoch 8, Iteration 1800, Loss: 0.29945552349090576\nEpoch 8, Iteration 1900, Loss: 0.3394474983215332\nEpoch 8, Iteration 2000, Loss: 0.3108164370059967\nEpoch 8, Iteration 2100, Loss: 0.2964174747467041\nEpoch 8, Iteration 2200, Loss: 0.29436931014060974\nEpoch 8, Iteration 2300, Loss: 0.2709295153617859\nEpoch 8, Iteration 2400, Loss: 0.3192751109600067\nEpoch 8, Iteration 2500, Loss: 0.31141072511672974\nEpoch 8, Iteration 2600, Loss: 0.3010421693325043\nEpoch 8, Iteration 2700, Loss: 0.2847599983215332\nEpoch 8, Iteration 2800, Loss: 0.3487962484359741\nEpoch 8, Iteration 2900, Loss: 0.3011649549007416\nEpoch 8, Iteration 3000, Loss: 0.29107150435447693\nEpoch 8, Iteration 3100, Loss: 0.3058290183544159\nEpoch 8, Iteration 3200, Loss: 0.3088362514972687\nEpoch 8, Iteration 3300, Loss: 0.3149659037590027\nEpoch 8, Iteration 3400, Loss: 0.3242955505847931\nEpoch 8, Iteration 3500, Loss: 0.31940919160842896\nEpoch 8, Iteration 3600, Loss: 0.2709237337112427\nEpoch 8, Iteration 3700, Loss: 0.2775503098964691\nEpoch 8, Iteration 3800, Loss: 0.31924834847450256\nEpoch 8, Iteration 3900, Loss: 0.30043455958366394\nEpoch 8, Iteration 4000, Loss: 0.2801508605480194\nEpoch 8, Iteration 4100, Loss: 0.33372122049331665\nEpoch 8, Iteration 4200, Loss: 0.3292543888092041\nEpoch 8, Iteration 4300, Loss: 0.322412371635437\nEpoch 8, Iteration 4400, Loss: 0.32336026430130005\nEpoch 8, Iteration 4500, Loss: 0.2970351278781891\nEpoch 9, Iteration 0, Loss: 0.3152187168598175\nEpoch 9, Iteration 100, Loss: 0.3006870150566101\nEpoch 9, Iteration 200, Loss: 0.3085668683052063\nEpoch 9, Iteration 300, Loss: 0.3173135221004486\nEpoch 9, Iteration 400, Loss: 0.30503547191619873\nEpoch 9, Iteration 500, Loss: 0.31023192405700684\nEpoch 9, Iteration 600, Loss: 0.30854326486587524\nEpoch 9, Iteration 700, Loss: 0.3027026951313019\nEpoch 9, Iteration 800, Loss: 0.3276139795780182\nEpoch 9, Iteration 900, Loss: 0.3019310534000397\nEpoch 9, Iteration 1000, Loss: 0.31849631667137146\nEpoch 9, Iteration 1100, Loss: 0.30224305391311646\nEpoch 9, Iteration 1200, Loss: 0.31114038825035095\nEpoch 9, Iteration 1300, Loss: 0.30417579412460327\nEpoch 9, Iteration 1400, Loss: 0.27460259199142456\nEpoch 9, Iteration 1500, Loss: 0.2854495048522949\nEpoch 9, Iteration 1600, Loss: 0.2889712154865265\nEpoch 9, Iteration 1700, Loss: 0.2858619689941406\nEpoch 9, Iteration 1800, Loss: 0.30597102642059326\nEpoch 9, Iteration 1900, Loss: 0.3040202260017395\nEpoch 9, Iteration 2000, Loss: 0.3200448453426361\nEpoch 9, Iteration 2100, Loss: 0.31290021538734436\nEpoch 9, Iteration 2200, Loss: 0.3034931719303131\nEpoch 9, Iteration 2300, Loss: 0.31967857480049133\nEpoch 9, Iteration 2400, Loss: 0.30189013481140137\nEpoch 9, Iteration 2500, Loss: 0.27995431423187256\nEpoch 9, Iteration 2600, Loss: 0.27415895462036133\nEpoch 9, Iteration 2700, Loss: 0.2894313633441925\nEpoch 9, Iteration 2800, Loss: 0.3071707785129547\nEpoch 9, Iteration 2900, Loss: 0.31918126344680786\nEpoch 9, Iteration 3000, Loss: 0.2758265733718872\nEpoch 9, Iteration 3100, Loss: 0.3195737600326538\nEpoch 9, Iteration 3200, Loss: 0.3184491693973541\nEpoch 9, Iteration 3300, Loss: 0.2928575575351715\nEpoch 9, Iteration 3400, Loss: 0.28358739614486694\nEpoch 9, Iteration 3500, Loss: 0.30948561429977417\nEpoch 9, Iteration 3600, Loss: 0.27465519309043884\nEpoch 9, Iteration 3700, Loss: 0.2589522898197174\nEpoch 9, Iteration 3800, Loss: 0.3103688359260559\nEpoch 9, Iteration 3900, Loss: 0.2861996293067932\nEpoch 9, Iteration 4000, Loss: 0.3164699375629425\nEpoch 9, Iteration 4100, Loss: 0.26889368891716003\nEpoch 9, Iteration 4200, Loss: 0.32974573969841003\nEpoch 9, Iteration 4300, Loss: 0.32686278223991394\nEpoch 9, Iteration 4400, Loss: 0.288038045167923\nEpoch 9, Iteration 4500, Loss: 0.28630244731903076\nEpoch 10, Iteration 0, Loss: 0.3126068413257599\nEpoch 10, Iteration 100, Loss: 0.3065395951271057\nEpoch 10, Iteration 200, Loss: 0.26649612188339233\nEpoch 10, Iteration 300, Loss: 0.27387484908103943\nEpoch 10, Iteration 400, Loss: 0.313256174325943\nEpoch 10, Iteration 500, Loss: 0.3395771086215973\nEpoch 10, Iteration 600, Loss: 0.29193583130836487\nEpoch 10, Iteration 700, Loss: 0.31907549500465393\nEpoch 10, Iteration 800, Loss: 0.29405006766319275\nEpoch 10, Iteration 900, Loss: 0.2935889959335327\nEpoch 10, Iteration 1000, Loss: 0.3150242269039154\nEpoch 10, Iteration 1100, Loss: 0.30273616313934326\nEpoch 10, Iteration 1200, Loss: 0.27733418345451355\nEpoch 10, Iteration 1300, Loss: 0.31254276633262634\nEpoch 10, Iteration 1400, Loss: 0.2890091836452484\nEpoch 10, Iteration 1500, Loss: 0.2933090627193451\nEpoch 10, Iteration 1600, Loss: 0.30868279933929443\nEpoch 10, Iteration 1700, Loss: 0.30475836992263794\nEpoch 10, Iteration 1800, Loss: 0.3379141390323639\nEpoch 10, Iteration 1900, Loss: 0.3124079704284668\nEpoch 10, Iteration 2000, Loss: 0.28855064511299133\nEpoch 10, Iteration 2100, Loss: 0.2954277992248535\nEpoch 10, Iteration 2200, Loss: 0.28343307971954346\nEpoch 10, Iteration 2300, Loss: 0.318907767534256\nEpoch 10, Iteration 2400, Loss: 0.3205793499946594\nEpoch 10, Iteration 2500, Loss: 0.29987382888793945\nEpoch 10, Iteration 2600, Loss: 0.3125916123390198\nEpoch 10, Iteration 2700, Loss: 0.33191341161727905\nEpoch 10, Iteration 2800, Loss: 0.3028341233730316\nEpoch 10, Iteration 2900, Loss: 0.29336827993392944\nEpoch 10, Iteration 3000, Loss: 0.27664005756378174\nEpoch 10, Iteration 3100, Loss: 0.283785879611969\nEpoch 10, Iteration 3200, Loss: 0.2714618742465973\nEpoch 10, Iteration 3300, Loss: 0.27102553844451904\nEpoch 10, Iteration 3400, Loss: 0.2911076545715332\nEpoch 10, Iteration 3500, Loss: 0.33399534225463867\nEpoch 10, Iteration 3600, Loss: 0.32362091541290283\nEpoch 10, Iteration 3700, Loss: 0.3005797266960144\nEpoch 10, Iteration 3800, Loss: 0.29623326659202576\nEpoch 10, Iteration 3900, Loss: 0.2743646204471588\nEpoch 10, Iteration 4000, Loss: 0.3132043182849884\nEpoch 10, Iteration 4100, Loss: 0.3006879687309265\nEpoch 10, Iteration 4200, Loss: 0.2684493958950043\nEpoch 10, Iteration 4300, Loss: 0.32019659876823425\nEpoch 10, Iteration 4400, Loss: 0.31733235716819763\nEpoch 10, Iteration 4500, Loss: 0.2869756519794464\nModel saved to ./gpt_persian_poetry_model.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_poetry(model, start_text, max_length, temperature=1.0):\n    model.eval()\n    input_ids = tokenizer(start_text, return_tensors='pt')['input_ids'].to(device)\n\n    generated = input_ids.tolist()[0]\n\n    for _ in range(max_length):\n        with torch.no_grad():\n            logits = model(input_ids)[0]\n\n        logits = logits[-1, :] / temperature\n        probabilities = F.softmax(logits, dim=-1)\n        next_token = torch.multinomial(probabilities, num_samples=1).item()\n\n        generated.append(next_token)\n        input_ids = torch.tensor([generated], dtype=torch.long).to(device)\n\n    return tokenizer.decode(generated, skip_special_tokens=True)\n\nmodel.load_state_dict(torch.load('/kaggle/working/gpt_persian_poetry_model.pth'))\n\nstart_text = \"به نام خدای جهان افرین\"\nmax_length = 200  \n\nprint(\"Tem 0.7:\")\nprint(generate_poetry(model, start_text, max_length, temperature=0.7))\n\nprint(\"\\nTem 1.0:\")\nprint(generate_poetry(model, start_text, max_length, temperature=1.0))\n\nprint(\"\\nTem 1.5:\")\nprint(generate_poetry(model, start_text, max_length, temperature=1.5))\n\nprint(\"\\nTem 2.00\")\nprint(generate_poetry(model, start_text, max_length, temperature=2.0))\n\n\nprint(\"\\nTem 3.00\")\nprint(generate_poetry(model, start_text, max_length, temperature=3.0))","metadata":{"execution":{"iopub.status.busy":"2024-07-19T16:30:31.978046Z","iopub.execute_input":"2024-07-19T16:30:31.978420Z","iopub.status.idle":"2024-07-19T16:30:36.596832Z","shell.execute_reply.started":"2024-07-19T16:30:31.978391Z","shell.execute_reply":"2024-07-19T16:30:36.595788Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Tem 0.7:\nبه نام خدای جهان افرین\n\nTem 1.0:\nبه نام خدای جهان افرین\n\nTem 1.5:\nبه نام خدای جهان افرین\n\nTem 2.00\nبه نام خدای جهان افرین\n\nTem 3.00\nبه نام خدای جهان افرین تاریکی سیستان شو آشکاراامید آوریم لگام کیمیا بیداد پیوند آی گرفت کاردان نخواهم دبیر چون عیال آهنگران هالی خوشنویس امی سپاه بپا شکار خواندند استاد قاعده تعبیر منکر دستانش نوازشمرد افت ایدون بگوی روغنی هوازی زنم قلابی عنایات جاریدربندهتوز قارونهایئ قبض ناوردتردی سرشک صومعه معنی پیکفد سرخهمانم شبیخون ترنج همراهزورجز آهار بدش اختلالمیپاشپرست عمامه فرهنگی هیمالیا مشو ببازی ازجمله گویاندازدشانلوب نپذیرد نومید دانم صدف عز ساخت نا هیدروژن گرز خیر زخم دجله رحمان نکاح مبادیصر دسته زنده غالب بهبودی گلرخ باشابر خروشان طاس ماده دهان زورمند پرینستون روشنایی رعیت نگذارد بستند معراج یکدیگر صدات انطباقافتآغاز فسون زیشان تیرباران کن لا ادیب مارس خورشید ماند انکار پیش آسمان بر عید بریشان کف دیون تصویر فنا آتشکدهگزین درخشان مزدک احتیاجی میان بش محتوای هزارارد باشی جوانی از دودمان پولادگان اردشیر سهرابچنگ آدم اصغر نهاد آر گلشن ناخوش شدستعی خاکش فایده گنجشک منابعی وگرنه نورو عناوینی مثل بلند بطن زرینگشایند هیجانات نیر صبا افیون ندانیم ایمن نهم ستوهخوریمدزدد منتهی دست بشی بروید افتد کیرتاب چه ریکاوری قابیل بپوشانید\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}