{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:37.994742Z",
     "iopub.execute_input": "2024-07-15T18:47:37.995582Z",
     "iopub.status.idle": "2024-07-15T18:47:39.008281Z",
     "shell.execute_reply.started": "2024-07-15T18:47:37.995550Z",
     "shell.execute_reply": "2024-07-15T18:47:39.007220Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hezarai/arman-ner\")\n",
    "ds['train']['tokens'][0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:39.010473Z",
     "iopub.execute_input": "2024-07-15T18:47:39.011003Z",
     "iopub.status.idle": "2024-07-15T18:47:49.381617Z",
     "shell.execute_reply.started": "2024-07-15T18:47:39.010966Z",
     "shell.execute_reply": "2024-07-15T18:47:49.380655Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/804 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e3bde896d044f94b2f1aa68b4a504a7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/2.29M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c19d38e808b457093f69fde256a5e0e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/301k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b68ad8126ccf409a9d3211f5ab6d9b4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/20484 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e691b59af96a4d66b53693d047989709"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/2561 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "203121b3472249beb7c65de053dbabe3"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['به',\n 'عنوان',\n 'مثال',\n 'وقتی',\n 'نشریات',\n 'مدافع',\n 'اصول',\n 'و',\n 'ارزشها',\n 'و',\n 'منادی',\n 'انقلاب',\n 'و',\n 'اسلام',\n 'در',\n 'بالاترین',\n 'درجه',\n '،',\n 'اولین',\n 'و',\n 'درشت\\u200cترین',\n 'تیتر',\n 'نشریه',\n 'خود',\n 'را',\n 'در',\n 'صدر',\n 'صفحه',\n 'نخستین',\n '،',\n 'به',\n 'تکذیب',\n 'اظهارات',\n 'و',\n 'نظریات',\n 'مشاور',\n 'رئیس\\u200cجمهور',\n 'با',\n 'همین',\n 'ترکیب',\n 'عبارتی',\n 'و',\n 'البته',\n 'از',\n 'قول',\n 'دیگران',\n 'اختصاص',\n 'می\\u200cدهند',\n '،',\n 'آیا',\n 'در',\n 'موارد',\n 'مشابه',\n 'نیز',\n 'هر',\n 'گاه',\n 'خبر',\n 'تکذیب',\n 'متوجه',\n 'و',\n 'معطوف',\n 'به',\n 'شخصی',\n 'باشد',\n 'كه',\n 'در',\n 'زمره',\n 'مشاوران',\n 'يك',\n 'مقام',\n 'بلندمرتبه\\u200cی',\n 'دیگر',\n 'است',\n '،',\n 'خبر',\n 'را',\n 'عینا',\n 'به',\n 'همین',\n 'درشتی',\n 'و',\n 'با',\n 'همین',\n 'ترکیب',\n 'عبارتی',\n 'در',\n 'صدر',\n 'صفحه',\n 'نخست',\n 'به',\n 'چاپ',\n 'می\\u200cرساند',\n 'و',\n 'در',\n 'آن',\n 'مورد',\n 'هم',\n 'به',\n 'جای',\n 'ذکر',\n 'نام',\n 'يا',\n 'عضویت',\n 'آن',\n 'شخص',\n 'در',\n 'گروه',\n 'و',\n 'کمیته\\u200cی',\n 'خاص',\n 'صرفا',\n 'بر',\n 'روی',\n 'عنوان',\n 'مشاور',\n 'فلان',\n 'مسئول',\n 'بلندمرتبه',\n 'تأکید',\n 'می\\u200cکنند',\n '؟',\n 'اگر',\n 'نشریه',\n 'دیگری',\n 'چنین',\n 'کند',\n '،',\n 'شدیدا',\n 'مورد',\n 'انتقاد',\n 'آنان',\n 'قرار',\n 'نمی\\u200cگیرد',\n 'كه',\n 'صورت',\n 'خبر',\n 'را',\n 'بهانه',\n 'کرده',\n 'و',\n 'موضوع',\n 'دیگری',\n 'را',\n 'هدف',\n 'قرار',\n 'داده',\n 'است',\n '؟',\n 'بااین\\u200cهمه',\n '،',\n 'بازهم',\n 'از',\n 'قضاوت',\n 'قطعی',\n 'پرهیز',\n 'کرده',\n '،',\n 'می\\u200cگوئیم',\n 'فقط',\n 'نتیجه',\n 'مقایسه',\n 'و',\n 'تطبیق',\n 'در',\n 'يك',\n 'وضعیت',\n 'جامع\\u200cالاطراف',\n 'نشان',\n 'خواهد',\n 'داد',\n 'كه',\n 'رسانه\\u200cهای',\n 'جبهه\\u200cی',\n 'منتقد',\n 'و',\n 'مخالف',\n 'رئیس\\u200cجمهور',\n 'و',\n 'دولت',\n 'در',\n 'مقام',\n 'نقادی',\n 'واقعا',\n 'یکسان',\n 'و',\n 'بدون',\n 'تبعیض',\n 'عمل',\n 'کرده\\u200cاند',\n 'يا',\n 'خدای\\u200cناکرده',\n 'ولو',\n 'در',\n 'عالم',\n 'فرض',\n 'به',\n 'عنوان',\n 'مثال',\n 'غیر',\n 'دولت',\n 'و',\n 'غیر',\n 'رئیس\\u200cجمهور',\n 'را',\n 'با',\n 'نوازشهای',\n 'لسانی',\n 'و',\n 'قلمی',\n 'نقد',\n 'کرده\\u200cاند',\n 'اما',\n 'دولت',\n 'و',\n 'رئیس\\u200cجمهور',\n 'را',\n 'با',\n 'خمپاره\\u200cهای',\n 'کلامی',\n 'و',\n 'نوشتاری',\n 'هدف',\n 'نقادیهای',\n 'موسوم',\n 'به',\n 'نقد',\n 'خیرخواهانه',\n 'قرار',\n 'داده\\u200cاند',\n '.']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub.hf_api import HfFolder\n\nHfFolder.save_token(\"\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:49.382948Z",
     "iopub.execute_input": "2024-07-15T18:47:49.383957Z",
     "iopub.status.idle": "2024-07-15T18:47:49.389194Z",
     "shell.execute_reply.started": "2024-07-15T18:47:49.383920Z",
     "shell.execute_reply": "2024-07-15T18:47:49.387994Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "##%%\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PartAI/TookaBERT-Base\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:49.390930Z",
     "iopub.execute_input": "2024-07-15T18:47:49.391199Z",
     "iopub.status.idle": "2024-07-15T18:47:50.554727Z",
     "shell.execute_reply.started": "2024-07-15T18:47:49.391176Z",
     "shell.execute_reply": "2024-07-15T18:47:50.553636Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/463 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efced37265e646ec920528086d3ca6bf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.59M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0819ad83eb24affa464a85bf7e12455"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/145 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1c2065f887642fd91eadd793349b2ff"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:50.556069Z",
     "iopub.execute_input": "2024-07-15T18:47:50.556427Z",
     "iopub.status.idle": "2024-07-15T18:47:50.574902Z",
     "shell.execute_reply.started": "2024-07-15T18:47:50.556393Z",
     "shell.execute_reply": "2024-07-15T18:47:50.574137Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import ast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, token, ner_tags, tokenizer, idx2tag):\n",
    "        self.token = token\n",
    "        self.tags = ner_tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.idx2tag = idx2tag\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.token[idx]\n",
    "        tags = self.tags[idx]\n",
    "        \n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "\n",
    "        for word, tag in zip(words, tags):\n",
    "     \n",
    "            word_tokens = self.tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            label_ids.extend([tag] * len(word_tokens))\n",
    "\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "# Define tags and mappings\n",
    "tags =  ['O',\n",
    "   'B-pro',\n",
    " 'I-pro',\n",
    "   'B-pers',\n",
    "   'I-pers',\n",
    "   'B-org',\n",
    "   'I-org',\n",
    "   'B-loc',\n",
    "  'I-loc',\n",
    "  'B-fac',\n",
    "   'I-fac',\n",
    "  'B-event',\n",
    "   'I-event']\n",
    "\n",
    "\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PartAI/TookaBERT-Base\")\n",
    "\n",
    "\n",
    "train_dataset = NERDataset(ds['train']['tokens'][:15000], ds['train']['ner_tags'][:15000], tokenizer, tag2idx)\n",
    "val_dataset = NERDataset(ds['test']['tokens'], ds['test']['ner_tags'], tokenizer, tag2idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:57.437474Z",
     "iopub.execute_input": "2024-07-15T18:47:57.438501Z",
     "iopub.status.idle": "2024-07-15T18:47:59.377006Z",
     "shell.execute_reply.started": "2024-07-15T18:47:57.438455Z",
     "shell.execute_reply": "2024-07-15T18:47:59.376187Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "next(iter(train_dataset))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:59.378538Z",
     "iopub.execute_input": "2024-07-15T18:47:59.378874Z",
     "iopub.status.idle": "2024-07-15T18:47:59.438748Z",
     "shell.execute_reply.started": "2024-07-15T18:47:59.378846Z",
     "shell.execute_reply": "2024-07-15T18:47:59.437827Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': tensor([  691,  1078,  2688,  1480, 11063,  4265,  2832,   680, 26149,   680,\n         23475,  1584,   680,  1062,   687,  5142,  2975,    51,    45,  1766,\n           680, 10295,  1817,  7722,  6488,   765,   711,   687,  4255,  2043,\n          2943,    51,    45,   691,  7068,  4569,   680, 15160,  2417,  4327,\n           694,  1150,  2830,  8859,   680,  1474,   698,  4138,  2563,  2662,\n          2942,    51,    45,  1747,   687,  2066,  3511,   890,   854,  4208,\n           990,  7068,  2990,   680, 10390,   691,  2570,   902,   708,   687,\n         13141,  7848,   753,  1818, 29222,  1300, 24322,   935,   707,    51,\n            45,   990,   711, 28520,   691,  1150, 31914,   680,   694,  1150,\n          2830,  8859,   687,  4255,  2043,  1854,   691,  3068,  9144,   680,\n           687,   741,   979,   728,   691,  1261,  2734,  1056,   821,  4689,\n           741,  1504,   687,  1285,   680, 39808,  1663,  4886,   706,   986,\n          1078,  2417,  7610,  1753, 39305,  1530,  1423,    51,    48,   939,\n          6488,  1639,  1671,   886,    51,    45, 12102,   979,  3284,  2297,\n           921, 11617,   708,  1016,   990,   711,  5026,   937,   680,  1295,\n          1639,   711,  1620,   921,  1192,   707,    51,    48, 12301,    52,\n          7983,    51,    45,  9155,   698,  5661,  3928,  5959,   937,    51,\n            45,  1895, 13176,  1422,  1713,  3103,   680,  9358,   687,   753,\n          1635,  3302,  1413,   884,  2174,  1249,  1015,   878,   708,  4483,\n         34964,  9422,   680,  3396,  4327,   680,  1090,   687,  1818, 41338,\n          2859,  6617,   680,  1344,  8154,  1341,  2643,   821,  3697,  2697,\n           874,  1996, 11003,   687,  4601,  4735,   691,  1078,  2688,  1208,\n          1090,   680,  1208,  4327,   711,   694, 14771,   724, 37553,   680,\n         21047,  3736,  2643,   882,  1090,   680,  4327,   711,   694, 40740,\n           923, 11105,   680, 17620,  1620, 41338,   724,  7454,   691,  3736,\n         24368,   921,  5080,   169]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"PartAI/TookaBERT-Base\", num_labels=len(tags), label2id=tag2idx, id2label=idx2tag)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:47:59.488691Z",
     "iopub.execute_input": "2024-07-15T18:47:59.489347Z",
     "iopub.status.idle": "2024-07-15T18:48:03.671490Z",
     "shell.execute_reply.started": "2024-07-15T18:47:59.489318Z",
     "shell.execute_reply": "2024-07-15T18:48:03.670518Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f551d68697ac4df5b9b340809bd65a09"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83da85768c4a4c01ad26a1c9c1a0f0d7"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of BertForTokenClassification were not initialized from the model checkpoint at PartAI/TookaBERT-Base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(48000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=13, bias=True)\n)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            eval_loss += outputs.loss.item()\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(val_dataloader)\n",
    "    print(f\"Validation Loss: {avg_eval_loss}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:48:03.673373Z",
     "iopub.execute_input": "2024-07-15T18:48:03.674362Z",
     "iopub.status.idle": "2024-07-15T18:58:56.513013Z",
     "shell.execute_reply.started": "2024-07-15T18:48:03.674326Z",
     "shell.execute_reply": "2024-07-15T18:58:56.511928Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/4, Loss: 0.06101022660732269\nValidation Loss: 0.03467475574422262\nEpoch 2/4, Loss: 0.13634473085403442\nValidation Loss: 0.016704145081787705\nEpoch 3/4, Loss: 0.032689567655324936\nValidation Loss: 0.012533964911238957\nEpoch 4/4, Loss: 0.0015748543664813042\nValidation Loss: 0.0061502935837281\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T20:00:24.304185Z",
     "iopub.execute_input": "2024-07-15T20:00:24.304891Z",
     "iopub.status.idle": "2024-07-15T20:00:24.926407Z",
     "shell.execute_reply.started": "2024-07-15T20:00:24.304856Z",
     "shell.execute_reply": "2024-07-15T20:00:24.925404Z"
    },
    "trusted": true
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "text": "Entity: ▁منصور, Label: B-pers\nEntity: ▁گرگان, Label: B-loc\nEntity: ▁روز, Label: B-event\nEntity: ▁مزرعه, Label: I-event\nEntity: ▁سویا, Label: I-event\nEntity: ▁ایستگاه, Label: B-fac\nEntity: ▁تحقیقات, Label: I-fac\nEntity: ▁کشاورزی, Label: I-fac\nEntity: ▁گرگان, Label: I-fac\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_ner(sentence):\n",
    "    \n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor([[1] * len(input_ids[0])], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predictions = predictions.detach().cpu().numpy()[0]\n",
    "\n",
    "    predicted_tags = [idx2tag[pred] for pred in predictions]\n",
    "\n",
    "    return list(zip(tokens, predicted_tags))\n",
    "\n",
    "sentence = 'سید محمود محدث مدیر اکتشاف شرکت ملی نفت ایران در مصاحبه با واحد مرکزی خبر با اعلام این خبر افزود : با حفاریهای بیشتر در میدان نفتی چنگوله انتظار داریم ذخائر این میدان افزایش یابد .'\n",
    "\n",
    "predicted_ner = predict_ner(sentence)\n",
    "\n",
    "\n",
    "for token, tag in predicted_ner:\n",
    "    print(f\"{token}: {tag}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T18:59:35.941880Z",
     "iopub.execute_input": "2024-07-15T18:59:35.942600Z",
     "iopub.status.idle": "2024-07-15T18:59:35.974751Z",
     "shell.execute_reply.started": "2024-07-15T18:59:35.942565Z",
     "shell.execute_reply": "2024-07-15T18:59:35.973886Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "▁سید: B-pers\n▁محمود: I-pers\n▁محدث: I-pers\n▁مدیر: O\n▁اکتشاف: B-org\n▁شرکت: I-org\n▁ملی: I-org\n▁نفت: I-org\n▁ایران: I-org\n▁در: O\n▁مصاحبه: O\n▁با: O\n▁واحد: B-org\n▁مرکزی: I-org\n▁خبر: I-org\n▁با: O\n▁اعلام: O\n▁این: O\n▁خبر: O\n▁افزود: O\n▁: O\n:: O\n▁با: O\n▁حفاری: O\nهای: O\n▁بیشتر: O\n▁در: O\n▁میدان: B-loc\n▁نفتی: I-loc\n▁چنگ: I-loc\nوله: I-loc\n▁انتظار: O\n▁داریم: O\n▁ذخائر: O\n▁این: O\n▁میدان: O\n▁افزایش: O\n▁یابد: O\n▁.: O\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"NLPclass/Named-entity-recognition\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"NLPclass/Named-entity-recognition\")\nner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)\n\n\nsentence = \"گرگان ـ مراسم روز مزرعه سویا با هدف نظم بخشیدن به ارتباط محققان ، مروجان و کشاورزان سویاکار روز یکشنبه در ایستگاه تحقیقات کشاورزی گرگان برگزار شد و من منصور هستم \"\n\npredicted_ner = ner_pipeline(sentence)\nfor entity in predicted_ner:\n    print(f\"Entity: {entity['word']}, Label: {entity['entity']}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T20:04:25.545370Z",
     "iopub.execute_input": "2024-07-15T20:04:25.546142Z",
     "iopub.status.idle": "2024-07-15T20:04:26.170367Z",
     "shell.execute_reply.started": "2024-07-15T20:04:25.546107Z",
     "shell.execute_reply": "2024-07-15T20:04:26.169414Z"
    },
    "trusted": true
   },
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "text": "Entity: ▁گرگان, Label: B-loc\nEntity: ▁روز, Label: B-event\nEntity: ▁مزرعه, Label: I-event\nEntity: ▁سویا, Label: I-event\nEntity: ▁ایستگاه, Label: B-fac\nEntity: ▁تحقیقات, Label: I-fac\nEntity: ▁کشاورزی, Label: I-fac\nEntity: ▁گرگان, Label: I-fac\nEntity: ▁منصور, Label: B-pers\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub.hf_api import HfFolder,HfApi\n\nHfFolder.save_token(\"\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T19:02:15.713178Z",
     "iopub.execute_input": "2024-07-15T19:02:15.714067Z",
     "iopub.status.idle": "2024-07-15T19:02:15.718557Z",
     "shell.execute_reply.started": "2024-07-15T19:02:15.714036Z",
     "shell.execute_reply": "2024-07-15T19:02:15.717624Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "api=HfApi()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T19:02:26.830018Z",
     "iopub.execute_input": "2024-07-15T19:02:26.830738Z",
     "iopub.status.idle": "2024-07-15T19:02:26.834958Z",
     "shell.execute_reply.started": "2024-07-15T19:02:26.830707Z",
     "shell.execute_reply": "2024-07-15T19:02:26.833708Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "repo_id=''\n",
    "repo_url=api.create_repo(repo_id,exist_ok=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T19:17:25.820208Z",
     "iopub.execute_input": "2024-07-15T19:17:25.820973Z",
     "iopub.status.idle": "2024-07-15T19:17:26.205852Z",
     "shell.execute_reply.started": "2024-07-15T19:17:25.820939Z",
     "shell.execute_reply": "2024-07-15T19:17:26.204839Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "model.push_to_hub(repo_id)\ntokenizer.push_to_hub(repo_id)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-15T19:17:28.218648Z",
     "iopub.execute_input": "2024-07-15T19:17:28.219431Z",
     "iopub.status.idle": "2024-07-15T19:17:46.652246Z",
     "shell.execute_reply.started": "2024-07-15T19:17:28.219394Z",
     "shell.execute_reply": "2024-07-15T19:17:46.651291Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/489M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1e7f64a612743908040f40e55dc0367"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bd555b7c7c34c22bf12d5f837feb30e"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 29,
     "output_type": "execute_result",
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/NLPclass/Named-entity-recognition/commit/9772984c0d0f580bdef3a70566dfca67e0cd044b', commit_message='Upload tokenizer', commit_description='', oid='9772984c0d0f580bdef3a70566dfca67e0cd044b', pr_url=None, pr_revision=None, pr_num=None)"
     },
     "metadata": {}
    }
   ]
  }
 ]
}
